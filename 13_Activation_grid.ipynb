{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ce770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from lucent.modelzoo import *\n",
    "from lucent.misc.io import show\n",
    "import lucent.optvis.objectives as objectives\n",
    "import lucent.optvis.param as param\n",
    "import lucent.optvis.render as render\n",
    "import lucent.optvis.transform as transform\n",
    "from lucent.misc.channel_reducer import ChannelReducer\n",
    "from lucent.misc.io import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class KittyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0 = nn.BatchNorm2d(3)\n",
    "        self.conv1 = nn.Conv2d(3, 9, 3)\n",
    "        self.pool1 = nn.AvgPool2d(4, 4)\n",
    "        \n",
    "        self.conv1_bn = nn.BatchNorm2d(9)\n",
    "        self.conv2 = nn.Conv2d(9, 16, 3)\n",
    "        self.pool2 = nn.AvgPool2d(4, 4)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 25, 3)\n",
    "        self.pool3 = nn.AvgPool2d(4, 4)\n",
    "        \n",
    "        self.conv3_bn = nn.BatchNorm2d(25)\n",
    "        self.conv4 = nn.Conv2d(25, 36, 3)\n",
    "        self.pool4 = nn.AvgPool2d(2 , 2)\n",
    "        \n",
    "        self.fc = nn.Linear(324, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = self.conv1_bn(self.pool1(F.relu(self.conv1(x))))\n",
    "        x = self.conv2_bn(self.pool2(F.relu(self.conv2(x))))\n",
    "        x = self.conv3_bn(self.pool3(F.relu(self.conv3(x))))\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class LongcatNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(3)\n",
    "        self.conv1 = nn.Conv2d(3, 9, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2_bn = nn.BatchNorm2d(9)\n",
    "        self.conv2 = nn.Conv2d(9, 16, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 25, 3)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv4_bn = nn.BatchNorm2d(25)\n",
    "        self.conv4 = nn.Conv2d(25, 36, 3)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv5_bn = nn.BatchNorm2d(36)\n",
    "        self.conv5 = nn.Conv2d(36, 36, 3)\n",
    "  \n",
    "        self.conv6_bn = nn.BatchNorm2d(36)\n",
    "        self.conv6 = nn.Conv2d(36, 49, 3)\n",
    "\n",
    "        self.conv7_bn = nn.BatchNorm2d(49)\n",
    "        self.conv7 = nn.Conv2d(49, 49, 3)\n",
    "        \n",
    "        self.conv8_bn = nn.BatchNorm2d(49)\n",
    "        self.conv8 = nn.Conv2d(49, 49, 3)\n",
    "        \n",
    "        self.conv9_bn = nn.BatchNorm2d(49)\n",
    "        self.conv9 = nn.Conv2d(49, 49, 3)\n",
    "        self.pool9 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv10_bn = nn.BatchNorm2d(49)\n",
    "        self.conv10 = nn.Conv2d(49, 49, 3)\n",
    "        self.pool10 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc = nn.Linear(1764, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2_bn(self.pool1(F.relu(self.conv1(x))))\n",
    "        x = self.conv3_bn(self.pool2(F.relu(self.conv2(x))))\n",
    "        x = self.conv4_bn(self.pool3(F.relu(self.conv3(x))))\n",
    "        x = self.conv5_bn(self.pool4(F.relu(self.conv4(x))))\n",
    "\n",
    "        x = self.conv6_bn(F.relu(self.conv5(x)))  \n",
    "        x = self.conv7_bn(F.relu(self.conv6(x)))\n",
    "        x = self.conv8_bn(F.relu(self.conv7(x)))\n",
    "        x = self.conv9_bn(F.relu(self.conv8(x)))\n",
    "        \n",
    "        \n",
    "        x = self.conv10_bn(self.pool9(F.relu(self.conv9(x))))\n",
    "        x = self.pool10(F.relu(self.conv10(x)))\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/activation_grids.ipynb\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_layer(model, layer, X):\n",
    "    hook = render.ModuleHook(getattr(model, layer))\n",
    "    model(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "\n",
    "@objectives.wrap_objective()\n",
    "def dot_compare(layer, acts, batch=1):\n",
    "    def inner(T):\n",
    "        pred = T(layer)[batch]\n",
    "        return -(pred * acts).sum(dim=0, keepdims=True).mean()\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "def render_activation_grid_less_naive(\n",
    "    img,\n",
    "    model,\n",
    "    layer=\"inception5b\",\n",
    "    cell_image_size=60,\n",
    "    n_groups=6,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "):\n",
    "    # First wee need, to normalize and resize the image\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    transforms = transform.standard_transforms.copy() + [\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        lambda x: x/255,\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    "    transforms_f = transform.compose(transforms)\n",
    "    # shape: (1, 3, original height of img, original width of img)\n",
    "    img = img.unsqueeze(0)\n",
    "    # shape: (1, 3, 224, 224)\n",
    "    img = transforms_f(img)\n",
    "\n",
    "    # Here we compute the activations of the layer `layer` using `img` as input\n",
    "    # shape: (layer_channels, layer_height, layer_width), the shape depends on the layer\n",
    "    acts = get_layer(model, layer, img)[0]\n",
    "    # shape: (layer_height, layer_width, layer_channels)\n",
    "    acts = acts.permute(1, 2, 0)\n",
    "    # shape: (layer_height*layer_width, layer_channels)\n",
    "    acts = acts.view(-1, acts.shape[-1])\n",
    "    acts_np = acts.cpu().numpy()\n",
    "    nb_cells = acts.shape[0]\n",
    "\n",
    "    # negative matrix factorization `NMF` is used to reduce the number\n",
    "    # of channels to n_groups. This will be used as the following.\n",
    "    # Each cell image in the grid is decomposed into a sum of\n",
    "    # (n_groups+1) images. First, each cell has its own set of parameters\n",
    "    #  this is what is called `cells_params` (see below). At the same time, we have\n",
    "    # a of group of images of size 'n_groups', which also have their own image parametrized\n",
    "    # by `groups_params`. The resulting image for a given cell in the grid\n",
    "    # is the sum of its own image (parametrized by `cells_params`)\n",
    "    # plus a weighted sum of the images of the group. Each each image from the group\n",
    "    # is weighted by `groups[cell_index, group_idx]`. Basically, this is a way of having\n",
    "    # the possibility to make cells with similar activations have a similar image, because\n",
    "    # cells with similar activations will have a similar weighting for the elements\n",
    "    # of the group.\n",
    "    if n_groups > 0:\n",
    "        reducer = ChannelReducer(n_groups, \"NMF\")\n",
    "        groups = reducer.fit_transform(acts_np)\n",
    "        groups /= groups.max(0)\n",
    "    else:\n",
    "        groups = np.zeros([])\n",
    "    # shape: (layer_height*layer_width, n_groups)\n",
    "    groups = torch.from_numpy(groups)\n",
    "\n",
    "    # Parametrization of the images of the groups (we have 'n_groups' groups)\n",
    "    groups_params, groups_image_f = param.fft_image(\n",
    "        [n_groups, 3, cell_image_size, cell_image_size]\n",
    "    )\n",
    "    # Parametrization of the images of each cell in the grid (we have 'layer_height*layer_width' cells)\n",
    "    cells_params, cells_image_f = param.fft_image(\n",
    "        [nb_cells, 3, cell_image_size, cell_image_size]\n",
    "    )\n",
    "\n",
    "    # First, we need to construct the images of the grid\n",
    "    # from the parameterizations\n",
    "\n",
    "    def image_f():\n",
    "        groups_images = groups_image_f()\n",
    "        cells_images = cells_image_f()\n",
    "        X = []\n",
    "        for i in range(nb_cells):\n",
    "            x = 0.7 * cells_images[i] + 0.5 * sum(\n",
    "                groups[i, j] * groups_images[j] for j in range(n_groups)\n",
    "            )\n",
    "            X.append(x)\n",
    "        X = torch.stack(X)\n",
    "        return X\n",
    "\n",
    "    # make sure the images are between 0 and 1\n",
    "    image_f = param.to_valid_rgb(image_f, decorrelate=True)\n",
    "\n",
    "    # After constructing the cells images, we sample randomly a mini-batch of cells\n",
    "    # from the grid. This is to prevent memory overflow, especially if the grid\n",
    "    # is large.\n",
    "    def sample(image_f, batch_size):\n",
    "        def f():\n",
    "            X = image_f()\n",
    "            inds = torch.randint(0, len(X), size=(batch_size,))\n",
    "            inputs = X[inds]\n",
    "            # HACK to store indices of the mini-batch, because we need them\n",
    "            # in objective func. Might be better ways to do that\n",
    "            sample.inds = inds\n",
    "            return inputs\n",
    "\n",
    "        return f\n",
    "\n",
    "    image_f_sampled = sample(image_f, batch_size=batch_size)\n",
    "\n",
    "    # Now, we define the objective function\n",
    "\n",
    "    def objective_func(model):\n",
    "        # shape: (batch_size, layer_channels, cell_layer_height, cell_layer_width)\n",
    "        pred = model(layer)\n",
    "        # use the sampled indices from `sample` to get the corresponding targets\n",
    "        target = acts[sample.inds].to(pred.device)\n",
    "        # shape: (batch_size, layer_channels, 1, 1)\n",
    "        target = target.view(target.shape[0], target.shape[1], 1, 1)\n",
    "        dot = (pred * target).sum(dim=1).mean()\n",
    "        return -dot\n",
    "\n",
    "    obj = objectives.Objective(objective_func)\n",
    "\n",
    "    def param_f():\n",
    "        # We optimize the parametrizations of both the groups and the cells\n",
    "        params = list(groups_params) + list(cells_params)\n",
    "        return params, image_f_sampled\n",
    "\n",
    "    results = render.render_vis(\n",
    "        model,\n",
    "        obj,\n",
    "        param_f,\n",
    "        thresholds=(n_steps,),\n",
    "        show_image=False,\n",
    "        progress=True,\n",
    "        fixed_image_size=cell_image_size,\n",
    "    )\n",
    "    # shape: (layer_height*layer_width, 3, grid_image_size, grid_image_size)\n",
    "    imgs = image_f()\n",
    "    imgs = imgs.cpu().data\n",
    "    imgs = imgs[:, :, 2:-2, 2:-2]\n",
    "    # turn imgs into a a grid\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=int(np.sqrt(nb_cells)), padding=0)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    grid = grid.numpy()\n",
    "    render.show(grid)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b26b5",
   "metadata": {},
   "source": [
    "## Kitty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(Image.open(\"dataset/test/LasVegas/0000075_0018745_0000004_0002564.jpg\"), np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb33924",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)\n",
    "#model.load_state_dict(torch.load('saved_models/inception/epoch_7_batch_5000.pth', map_location=device))\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_activation_grid_less_naive(\n",
    "            img, model, 'inception4b', cell_image_size=120, n_steps=1024, batch_size=64, n_groups=8\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inception5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba14779",
   "metadata": {},
   "outputs": [],
   "source": [
    "kitty = KittyNet()\n",
    "kitty.load_state_dict(torch.load('saved_models/kitty/epoch_7_batch_5000.pth', map_location=device))\n",
    "kitty.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e186a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,50):\n",
    "    try:\n",
    "        render_activation_grid_less_naive(\n",
    "            img, kitty, 'conv3', cell_image_size=x, n_steps=1024, batch_size=64, n_groups=2\n",
    "            )\n",
    "        print(\"aloha\")\n",
    "    except RuntimeError:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Longcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inception"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
